{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: unicode -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\insuf\\AppData\\Local\\conda\\conda\\envs\\python352\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from matplotlib import pyplot\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gensim.models.doc2vec.FAST_VERSION > -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\insuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\insuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13283795747212513169\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3129039257\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 4024810860380551380\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_clean.csv')\n",
    "train.comment_text = train.comment_text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_train = train[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223043"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_sentences_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = train.comment_text.values.tolist()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Function to filter stop words from tokens for each sentence\n",
    "def sentence_filter(sentence_token, stop_words):\n",
    "    filtered = []\n",
    "    for token in sentence_token:\n",
    "        if not token in stop_words:\n",
    "            filtered.append(token)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "# Input description, tokenize each sentence and return token for each sentence without stopping words\n",
    "def sentence_tokenizer(description, stop_words):\n",
    "    value = []\n",
    "    for sentence in description:\n",
    "        sentence = sentence.lower()\n",
    "        tokenizer_ = nltk.tokenize.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "        sentence_token = tokenizer_.tokenize(sentence)\n",
    "        # sentence_token = nltk.word_tokenize(sentence)\n",
    "        filtered_token = sentence_filter(sentence_token, stop_words)\n",
    "        value.append(filtered_token)\n",
    "    return value\n",
    "\n",
    "\n",
    "# Tokenize the sentence\n",
    "description_token = sentence_tokenizer(description, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip gram model with minimum word count = 10 and output vector of length 300\n",
    "# model_sg = gensim.models.Word2Vec(description_token, window=2, min_count=10, size=300, workers=4, seed=123, iter=10)\n",
    "\n",
    "# CBOW(continuous bag of words) model\n",
    "model_cbow = gensim.models.Word2Vec(description_token, window=2, min_count=10, size=300, workers=4, seed=123, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg.save('model_sg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow.save('model_cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25653 word vectors.\n",
      "total embedded: 19861 common words\n"
     ]
    }
   ],
   "source": [
    "model_sg = gensim.models.Word2Vec.load('model_sg')\n",
    "\n",
    "embed_size = 300\n",
    "embeddings_index = dict()\n",
    "for word in model_sg.wv.vocab:\n",
    "    embeddings_index[word] = model_sg.wv[word]\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "nb_words = min(max_features, len(tokenizer.word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "\n",
    "embeddedCount = 0\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        embeddedCount += 1\n",
    "print('total embedded:',embeddedCount,'common words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25653 word vectors.\n",
      "total embedded: 19861 common words\n"
     ]
    }
   ],
   "source": [
    "model_cbow = gensim.models.Word2Vec.load('model_cbow')\n",
    "\n",
    "embed_size = 300\n",
    "embeddings_index = dict()\n",
    "for word in model_cbow.wv.vocab:\n",
    "    embeddings_index[word] = model_cbow.wv[word]\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "nb_words = min(max_features, len(tokenizer.word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "\n",
    "embeddedCount = 0\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        embeddedCount += 1\n",
    "print('total embedded:',embeddedCount,'common words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fucking', 0.5763519406318665),\n",
       " ('fuk', 0.5572361946105957),\n",
       " ('bitch', 0.5500202178955078),\n",
       " ('fuckin', 0.5281999707221985),\n",
       " ('fck', 0.5242219567298889),\n",
       " ('cocksucker', 0.5240716934204102),\n",
       " ('jerk', 0.517245352268219),\n",
       " ('fucker', 0.511810302734375),\n",
       " ('fuckwit', 0.5083379745483398),\n",
       " ('retards', 0.5072153806686401)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbow.wv.most_similar('fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "     auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "     K.get_session().run(tf.local_variables_initializer())\n",
    "     return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 120)          173280    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 6,179,636\n",
      "Trainable params: 6,179,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,embed_size,weights=[embedding_matrix],input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(60,return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(6, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[auc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "for item in y.tolist():\n",
    "    for value in item:\n",
    "        Y.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0, 1: 7.967913497755453}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_class_weights(y, smooth_factor=0):\n",
    "    \"\"\"\n",
    "    Returns the weights for each class based on the frequencies of the samples\n",
    "    :param smooth_factor: factor that smooths extremely uneven weights\n",
    "    :param y: list of true labels (the labels must be hashable)\n",
    "    :return: dictionary with the weight for each class\n",
    "    \"\"\"\n",
    "    counter = Counter(y)\n",
    "\n",
    "    if smooth_factor > 0:\n",
    "        p = max(counter.values()) * smooth_factor\n",
    "        for k in counter.keys():\n",
    "            counter[k] += p\n",
    "\n",
    "    majority = max(counter.values())\n",
    "\n",
    "    return {cls: float(majority / count) for cls, count in counter.items()}\n",
    "\n",
    "get_class_weights(Y, smooth_factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/4\n",
      "127656/127656 [==============================] - 213s 2ms/step - loss: 0.0831 - auc: 0.8498 - val_loss: 0.0489 - val_auc: 0.9517\n",
      "Epoch 2/4\n",
      "127656/127656 [==============================] - 210s 2ms/step - loss: 0.0468 - auc: 0.9640 - val_loss: 0.0458 - val_auc: 0.9703\n",
      "Epoch 3/4\n",
      "127656/127656 [==============================] - 210s 2ms/step - loss: 0.0420 - auc: 0.9740 - val_loss: 0.0464 - val_auc: 0.9762\n",
      "Epoch 4/4\n",
      "127656/127656 [==============================] - 206s 2ms/step - loss: 0.0380 - auc: 0.9780 - val_loss: 0.0468 - val_auc: 0.9794\n"
     ]
    }
   ],
   "source": [
    "tensorBoardCB = TensorBoard(log_dir='./Graph', histogram_freq=0,  \n",
    "          write_graph=True, write_images=True)\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    batch_size_ = 256\n",
    "    epochs_ = 4\n",
    "#     class_weight_ = get_class_weights(Y, smooth_factor=0.1)\n",
    "    history = model.fit(X_t,y,batch_size=batch_size_,epochs=epochs_,validation_split=0.2, callbacks=[tensorBoardCB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(history.history['acc'])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('test_clean.csv')\n",
    "# test.comment_text = test.comment_text.astype(str)\n",
    "# list_sentences_test = test[\"comment_text\"]\n",
    "# list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "# X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "# y_test = model.predict([X_te], batch_size=512, verbose=1)\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
